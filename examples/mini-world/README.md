# Glia Example: Event-Camera Mini-World

This directory provides a **self-contained environment** for generating, visualizing, and training spiking neural networks on _synthetic event-camera data_.
It simulates a small 2-D â€œmini-worldâ€ observed through a **Dynamic Vision Sensor (DVS)**, producing realistic ON/OFF event streams suitable for Glia input.

The goal is to test how well the Glia framework can:

- **Perceive motion and object presence** using spatiotemporal spikes rather than static images.
- **Integrate over time** to estimate or count moving objects.
- **Generalize to new event patterns** generated by controlled world physics.

---

## ğŸ§  Problem Overview

Unlike conventional cameras, an **event camera** outputs asynchronous brightness changes instead of full image frames.
Each pixel fires events whenever its intensity changes â€” yielding a sparse, time-encoded representation of motion.

In this mini-world simulation:

- Bright objects move over a darker background.
- Each pixel produces **ON events** when it brightens and **OFF events** when it darkens.
- The generator aggregates these into temporal **spike sequences** compatible with Glia (`.seq` format).
- The corresponding label is the **object count** (or other chosen variable) per time step.

This dataset allows Glia to learn dynamic scene representations such as:

- Counting or classifying moving objects.
- Detecting motion direction or velocity.
- Building temporal memory from sequences of spikes.

The simulation serves as a foundation for benchmarking biologically inspired networks on temporal perception tasks â€” all without requiring real hardware.

---

## ğŸ§© Directory Layout

```
examples/
â””â”€â”€ mini-world/
    â”œâ”€â”€ generator/
    â”‚   â”œâ”€â”€ event_world.py         # Main data generator (produces npz + .seq + labels)
    â”‚   â”œâ”€â”€ visualize_world.py     # Static visualizer (frames + event rasters)
    â”‚   â”œâ”€â”€ animate_world.py       # Animated playback with HUD + progress bar
    â”‚   â””â”€â”€ utils/                 # Helper functions (optional extensions)
    â”‚
    â”œâ”€â”€ data/
    â”‚   â””â”€â”€ run1/                  # Example output from generator
    â”‚       â”œâ”€â”€ seq/               # Glia-ready .seq inputs
    â”‚       â”œâ”€â”€ labels/            # Ground-truth labels (e.g., counts)
    â”‚       â”œâ”€â”€ npz/               # Raw simulator frames and event tensors
    â”‚       â””â”€â”€ config.json        # Configuration snapshot for reproducibility
    â”‚
    |â”€â”€ env_config.md              # Simulator parameters and file format details
    â”‚â”€â”€ baseline_network.md        # Reference Glia network topology
    â””â”€â”€ README.txt                 # (this file)
```

---

## âš™ï¸ Workflow Summary

### 1. Generate data

```
python generator/event_world.py --out data/run1 --seed 1 --clips 10 --width 32 --height 32 --max-objects 3
```

Produces:

- `seq/clip_00000.seq` â€¦ Glia-readable input event streams
- `labels/labels_counts.csv` â€¦ true per-tick object counts
- `npz/` + `config.json` for reproducibility

### 2. Visualize or animate

```
python generator/animate_world.py --in data/run1 --clip 0 --show-events --theme dark
```

Displays a moving scene with HUD and progress bar showing tick/time progress.

### 3. Train and evaluate in Glia

Use the `.seq` inputs with the **baseline network** defined in  
[`docs/baseline_network.md`](./docs/baseline_network.md).  
Evaluation scripts can read the predicted counts and produce confusion matrices.

---

## ğŸ§­ Configuration and Network Docs

- **Simulation settings and data format:**  
  â†’ [`docs/env_config.md`](./docs/env_config.md)

- **Baseline network structure and training strategy:**  
  â†’ [`docs/baseline_network.md`](./docs/baseline_network.md)

- **Additional example environments (advanced):**  
  â†’ [`docs/examples/`](./docs/examples)

---

## ğŸ’¡ Extensions

The same framework can be extended to:

- Multi-agent motion (Predatorâ€“Prey).
- Audio onset streams (Audio-Rhythm).
- Physical simulations (CartPole).
- Sensory fusion (IMU gestures).
- Abstract dynamics (Cellular Automata).

Each variant is documented in `docs/examples/` with suggested network modifications.

---

## ğŸ§± Summary

This example demonstrates the **end-to-end Glia pipeline**:

- Synthetic world generation â†’ event stream creation â†’ spiking network inference â†’ supervised evaluation.

Itâ€™s an ideal sandbox for experimenting with:

- Spatiotemporal encoding,
- Neural plasticity rules,
- Integration and prediction over time.

The mini-world can be expanded into richer multimodal simulations to benchmark neuromorphic learning at scale.
