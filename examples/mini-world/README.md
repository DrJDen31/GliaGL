# Glia Example: Event-Camera Mini-World

This directory provides a **self-contained environment** for generating, visualizing, and training spiking neural networks on _synthetic event-camera data_.
It simulates a small 2-D “mini-world” observed through a **Dynamic Vision Sensor (DVS)**, producing realistic ON/OFF event streams suitable for Glia input.

The goal is to test how well the Glia framework can:

- **Perceive motion and object presence** using spatiotemporal spikes rather than static images.
- **Integrate over time** to estimate or count moving objects.
- **Generalize to new event patterns** generated by controlled world physics.

---

## 🧠 Problem Overview

Unlike conventional cameras, an **event camera** outputs asynchronous brightness changes instead of full image frames.
Each pixel fires events whenever its intensity changes — yielding a sparse, time-encoded representation of motion.

In this mini-world simulation:

- Bright objects move over a darker background.
- Each pixel produces **ON events** when it brightens and **OFF events** when it darkens.
- The generator aggregates these into temporal **spike sequences** compatible with Glia (`.seq` format).
- The corresponding label is the **object count** (or other chosen variable) per time step.

This dataset allows Glia to learn dynamic scene representations such as:

- Counting or classifying moving objects.
- Detecting motion direction or velocity.
- Building temporal memory from sequences of spikes.

The simulation serves as a foundation for benchmarking biologically inspired networks on temporal perception tasks — all without requiring real hardware.

---

## 🧩 Directory Layout

```
examples/
└── mini-world/
    ├── generator/
    │   ├── event_world.py         # Main data generator (produces npz + .seq + labels)
    │   ├── visualize_world.py     # Static visualizer (frames + event rasters)
    │   ├── animate_world.py       # Animated playback with HUD + progress bar
    │   └── utils/                 # Helper functions (optional extensions)
    │
    ├── data/
    │   └── run1/                  # Example output from generator
    │       ├── seq/               # Glia-ready .seq inputs
    │       ├── labels/            # Ground-truth labels (e.g., counts)
    │       ├── npz/               # Raw simulator frames and event tensors
    │       └── config.json        # Configuration snapshot for reproducibility
    │
    |── env_config.md              # Simulator parameters and file format details
    │── baseline_network.md        # Reference Glia network topology
    └── README.txt                 # (this file)
```

---

## ⚙️ Workflow Summary

### 1. Generate data

```
python generator/event_world.py --out data/run1 --seed 1 --clips 10 --width 32 --height 32 --max-objects 3
```

Produces:

- `seq/clip_00000.seq` … Glia-readable input event streams
- `labels/labels_counts.csv` … true per-tick object counts
- `npz/` + `config.json` for reproducibility

### 2. Visualize or animate

```
python generator/animate_world.py --in data/run1 --clip 0 --show-events --theme dark
```

Displays a moving scene with HUD and progress bar showing tick/time progress.

### 3. Train and evaluate in Glia

Use the `.seq` inputs with the **baseline network** defined in  
[`docs/baseline_network.md`](./docs/baseline_network.md).  
Evaluation scripts can read the predicted counts and produce confusion matrices.

---

## 🧭 Configuration and Network Docs

- **Simulation settings and data format:**  
  → [`docs/env_config.md`](./docs/env_config.md)

- **Baseline network structure and training strategy:**  
  → [`docs/baseline_network.md`](./docs/baseline_network.md)

- **Additional example environments (advanced):**  
  → [`docs/examples/`](./docs/examples)

---

## 💡 Extensions

The same framework can be extended to:

- Multi-agent motion (Predator–Prey).
- Audio onset streams (Audio-Rhythm).
- Physical simulations (CartPole).
- Sensory fusion (IMU gestures).
- Abstract dynamics (Cellular Automata).

Each variant is documented in `docs/examples/` with suggested network modifications.

---

## 🧱 Summary

This example demonstrates the **end-to-end Glia pipeline**:

- Synthetic world generation → event stream creation → spiking network inference → supervised evaluation.

It’s an ideal sandbox for experimenting with:

- Spatiotemporal encoding,
- Neural plasticity rules,
- Integration and prediction over time.

The mini-world can be expanded into richer multimodal simulations to benchmark neuromorphic learning at scale.
